name: Monthly A11y + Lighthouse Reports

on:
  schedule:
    # Runs at 06:00 UTC on the 1st of every month
    - cron: "0 6 1 * *"
  workflow_dispatch:
    inputs:
      sitemap_url:
        description: "Sitemap URL (xml or xml.gz). Used if single_url is empty. Leave empty to use env.SITEMAP_URL"
        required: false
        default: ""
      single_url:
        description: "Single URL to audit (overrides sitemap_url when provided)"
        required: false
        default: ""
      max_urls:
        description: "Optional cap for sitemap-based runs (0 = no cap). Ignored when single_url is provided."
        required: false
        default: "0"

permissions:
  contents: read

concurrency:
  group: monthly-a11y-perf
  cancel-in-progress: false

jobs:
  audit:
    runs-on: ubuntu-latest

    env:
      # Set your default sitemap here (can be overridden by workflow_dispatch input)
      SITEMAP_URL: "https://example.com/sitemap.xml"
      # Optional global cap (can be overridden by workflow_dispatch input)
      MAX_URLS: "0"

      # Puppeteer/Chrome sandbox flags (fixes the common "No usable sandbox" crash on GitHub runners)
      CHROME_FLAGS: "--no-sandbox --disable-setuid-sandbox"

    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Node
        uses: actions/setup-node@v4
        with:
          node-version: 20
          cache: "npm"

      - name: Install tooling (pa11y + lhci + xml parser)
        run: |
          npm ci || true
          npm install --no-save pa11y-ci pa11y-ci-reporter-html @lhci/cli fast-xml-parser

      - name: Resolve inputs/env
        id: vars
        run: |
          SITEMAP="${{ inputs.sitemap_url }}"
          if [ -z "$SITEMAP" ]; then SITEMAP="${SITEMAP_URL}"; fi

          SINGLE="${{ inputs.single_url }}"
          if [ -z "$SINGLE" ] && [ -n "${SINGLE_URL:-}" ]; then SINGLE="${SINGLE_URL}"; fi

          MAX="${{ inputs.max_urls }}"
          if [ -z "$MAX" ]; then MAX="${MAX_URLS}"; fi

          echo "sitemap=$SITEMAP" >> $GITHUB_OUTPUT
          echo "single_url=$SINGLE" >> $GITHUB_OUTPUT
          echo "max_urls=$MAX" >> $GITHUB_OUTPUT

      - name: Build URL list (sitemap or single url) -> urls.txt
        run: |
          node <<'NODE'
          const fs = require("fs");
          const zlib = require("zlib");
          const { XMLParser } = require("fast-xml-parser");

          const sitemapUrl = process.env.SITEMAP_URL_EFFECTIVE || "";
          const singleUrl = (process.env.SINGLE_URL_EFFECTIVE || "").trim();
          const maxUrls = Number(process.env.MAX_URLS_EFFECTIVE || "0");

          function assertHttpUrl(u) {
            try {
              const x = new URL(u);
              if (x.protocol !== "http:" && x.protocol !== "https:") throw new Error("Only http/https allowed");
            } catch {
              throw new Error(`Invalid URL: ${u}`);
            }
          }

          if (singleUrl) {
            assertHttpUrl(singleUrl);
            fs.writeFileSync("urls.txt", singleUrl + "\n", "utf8");
            fs.writeFileSync("urls.json", JSON.stringify([singleUrl], null, 2), "utf8");
            console.log(`Using single URL: ${singleUrl}`);
            process.exit(0);
          }

          if (!sitemapUrl) throw new Error("No single_url provided and no sitemap URL available.");

          const parser = new XMLParser({ ignoreAttributes: false, allowBooleanAttributes: true });

          async function fetchTextOrGunzip(url) {
            const res = await fetch(url, { redirect: "follow" });
            if (!res.ok) throw new Error(`Failed to fetch ${url} (${res.status})`);
            const buf = Buffer.from(await res.arrayBuffer());
            if (url.endsWith(".gz")) return zlib.gunzipSync(buf).toString("utf8");
            return buf.toString("utf8");
          }

          function toArray(x) {
            if (!x) return [];
            return Array.isArray(x) ? x : [x];
          }

          async function extractFromUrl(url) {
            const xml = await fetchTextOrGunzip(url);
            const data = parser.parse(xml);

            if (data.urlset && data.urlset.url) {
              const urls = toArray(data.urlset.url).map(u => u.loc).filter(Boolean).map(String);
              return { type: "urlset", urls };
            }

            if (data.sitemapindex && data.sitemapindex.sitemap) {
              const sitemaps = toArray(data.sitemapindex.sitemap).map(s => s.loc).filter(Boolean).map(String);
              return { type: "index", sitemaps };
            }

            throw new Error(`Unrecognized sitemap format at ${url}`);
          }

          (async () => {
            assertHttpUrl(sitemapUrl);

            const seen = new Set();
            const out = [];

            async function addUrls(urls) {
              for (const u of urls) {
                if (!u) continue;
                try { assertHttpUrl(u); } catch { continue; }
                if (!seen.has(u)) {
                  seen.add(u);
                  out.push(u);
                  if (maxUrls > 0 && out.length >= maxUrls) return false;
                }
              }
              return true;
            }

            const root = await extractFromUrl(sitemapUrl);

            if (root.type === "urlset") {
              await addUrls(root.urls);
            } else {
              for (const sm of root.sitemaps) {
                const child = await extractFromUrl(sm);
                if (child.type !== "urlset") continue;
                const ok = await addUrls(child.urls);
                if (!ok) break;
              }
            }

            if (out.length === 0) throw new Error("Sitemap produced 0 valid URLs.");

            fs.writeFileSync("urls.txt", out.join("\n") + "\n", "utf8");
            fs.writeFileSync("urls.json", JSON.stringify(out, null, 2), "utf8");
            console.log(`Extracted ${out.length} URLs from sitemap`);
          })().catch(err => {
            console.error(err);
            process.exit(1);
          });
          NODE
        env:
          SITEMAP_URL_EFFECTIVE: ${{ steps.vars.outputs.sitemap }}
          SINGLE_URL_EFFECTIVE: ${{ steps.vars.outputs.single_url }}
          MAX_URLS_EFFECTIVE: ${{ steps.vars.outputs.max_urls }}

      - name: Prepare report folders
        run: |
          mkdir -p reports/pa11y
          mkdir -p reports/lighthouse
          mkdir -p reports/meta

      - name: Create Pa11y config from urls.txt
        run: |
          node <<'NODE'
          const fs = require("fs");

          const urls = fs.readFileSync("urls.txt","utf8")
            .split("\n").map(s=>s.trim()).filter(Boolean);

          const cfg = {
            defaults: {
              timeout: 60000,
              chromeLaunchConfig: {
                args: (process.env.CHROME_FLAGS || "").split(" ").filter(Boolean),
              },
            },
            urls
          };

          fs.writeFileSync(".pa11yci.json", JSON.stringify(cfg, null, 2), "utf8");
          console.log(`Pa11y config created with ${urls.length} URLs`);
          NODE
        env:
          CHROME_FLAGS: ${{ env.CHROME_FLAGS }}

      - name: Run Pa11y (JSON + HTML) and always keep reports
        run: |
          set +e

          # JSON
          npx pa11y-ci -c .pa11yci.json --threshold 999999 --reporter json \
            > reports/pa11y/pa11y-report.json \
            2> reports/pa11y/pa11y-json.stderr.log
          PA11Y_JSON_EXIT=$?
          echo "$PA11Y_JSON_EXIT" > reports/pa11y/pa11y-json.exitcode

          if [ "$PA11Y_JSON_EXIT" -ne 0 ]; then
            echo "Pa11y JSON exited with code: $PA11Y_JSON_EXIT"
            if [ -s reports/pa11y/pa11y-json.stderr.log ]; then
              echo "---- Pa11y JSON stderr (first 200 lines) ----"
              sed -n '1,200p' reports/pa11y/pa11y-json.stderr.log
            else
              echo "---- Pa11y JSON stderr is empty. Showing first 200 lines of stdout file ----"
              sed -n '1,200p' reports/pa11y/pa11y-report.json || true
            fi
          fi

          # HTML
          npx pa11y-ci -c .pa11yci.json --threshold 999999 --reporter pa11y-ci-reporter-html \
            > reports/pa11y/pa11y-report.html \
            2> reports/pa11y/pa11y-html.stderr.log
          PA11Y_HTML_EXIT=$?
          echo "$PA11Y_HTML_EXIT" > reports/pa11y/pa11y-html.exitcode

          if [ "$PA11Y_HTML_EXIT" -ne 0 ]; then
            echo "Pa11y HTML exited with code: $PA11Y_HTML_EXIT"
            if [ -s reports/pa11y/pa11y-html.stderr.log ]; then
              echo "---- Pa11y HTML stderr (first 200 lines) ----"
              sed -n '1,200p' reports/pa11y/pa11y-html.stderr.log
            else
              echo "---- Pa11y HTML stderr is empty. (stdout is the HTML report file) ----"
              echo "HTML report path: reports/pa11y/pa11y-report.html"
            fi
          fi

          # Never fail the workflow on pa11y (we want the monthly artifact no matter what)
          exit 0

      - name: Create Lighthouse CI config from urls.txt
        run: |
          node <<'NODE'
          const fs = require("fs");

          const urls = fs.readFileSync("urls.txt","utf8")
            .split("\n").map(s=>s.trim()).filter(Boolean);

          const chromeFlags = (process.env.CHROME_FLAGS || "").split(" ").filter(Boolean);

          const cfg = {
            ci: {
              collect: {
                url: urls,
                numberOfRuns: 1,
                settings: { chromeFlags }
              },
              upload: {
                target: "filesystem",
                outputDir: "reports/lighthouse"
              }
            }
          };

          fs.writeFileSync("lighthouserc.json", JSON.stringify(cfg, null, 2), "utf8");
          console.log(`LHCI config created with ${urls.length} URLs`);
          NODE
        env:
          CHROME_FLAGS: ${{ env.CHROME_FLAGS }}

      - name: Run Lighthouse CI (HTML + JSON artifacts)
        continue-on-error: true
        run: |
          npx lhci autorun --config=./lighthouserc.json || true

          if [ -d ".lighthouseci" ]; then
            mkdir -p reports/lighthouse/raw
            cp -R .lighthouseci/* reports/lighthouse/raw/ || true
          fi

      - name: Copy URL lists into reports for convenience
        run: |
          cp urls.txt reports/meta/urls.txt
          cp urls.json reports/meta/urls.json

      - name: Zip everything into a single file
        if: always()
        run: |
          TS="$(date -u +'%Y-%m')"
          ZIP="monthly-reports-${TS}.zip"
          zip -r "$ZIP" reports > /dev/null
          echo "ZIP_NAME=$ZIP" >> $GITHUB_ENV

      - name: Upload artifact (single zip)
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: monthly-a11y-lighthouse-reports
          path: ${{ env.ZIP_NAME }}
          retention-days: 30
